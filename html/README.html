

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SignLink-ISL &mdash; SIGNLINK-ISL 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=2709fde1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Utils" href="utils/index.html" />
    <link rel="prev" title="SIGNLINK-ISL documentation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            SIGNLINK-ISL
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">SignLink-ISL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#table-of-contents">Table of Contents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#approach">Approach</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dataset-preparation-and-processing">Dataset Preparation and Processing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#include-subset-50-creation">INCLUDE SUBSET 50 Creation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dataset-loader-versions">Dataset Loader Versions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-augmentation">Data Augmentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-exploration">Model Exploration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#challenges-faced">Challenges Faced</a></li>
<li class="toctree-l3"><a class="reference internal" href="#future-improvements">Future Improvements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-performance">Model Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#include-subset-50">INCLUDE SUBSET 50</a></li>
<li class="toctree-l3"><a class="reference internal" href="#complete-include-dataset">Complete INCLUDE Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#installation">Installation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dataset-references">Dataset &amp; References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils/index.html">Utils</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">SIGNLINK-ISL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">SignLink-ISL</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="signlink-isl">
<h1>SignLink-ISL<a class="headerlink" href="#signlink-isl" title="Link to this heading">ÔÉÅ</a></h1>
<p>SignLink-ISL is a deep learning-based system designed to recognize and interpret Indian Sign Language (ISL) gestures, facilitating effective communication between the deaf community and others. This project aims to build a robust Sign Language Recognition (SLR) model, with a specific focus on Indian Sign Language (ISL). The goal is to preprocess video datasets efficiently and train machine learning models to recognize and classify ISL gestures accurately.</p>
<section id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Link to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#introduction"><span class="xref myst">Introduction</span></a></p></li>
<li><p><a class="reference internal" href="#approach"><span class="xref myst">Approach</span></a></p></li>
<li><p><a class="reference internal" href="#model-performance"><span class="xref myst">Model Performance</span></a></p></li>
<li><p><a class="reference internal" href="#installation"><span class="xref myst">Installation</span></a></p></li>
<li><p><a class="reference internal" href="#usage"><span class="xref myst">Usage</span></a></p></li>
<li><p><a class="reference internal" href="#dataset--references"><span class="xref myst">Dataset &amp; References</span></a></p></li>
</ul>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">ÔÉÅ</a></h2>
<p>Effective communication with individuals who are deaf or hard of hearing is increasingly important. To address this need, we developed SignLink-ISL. While the system is currently limited to recognizing and translating isolated sign language words due to dataset constraints, it lays the groundwork for future advancements. We hope this project serves as a starting point for others to build upon, eventually leading to a real-time translation system capable of converting entire sign language sentences into spoken language.</p>
</section>
<section id="approach">
<h2>Approach<a class="headerlink" href="#approach" title="Link to this heading">ÔÉÅ</a></h2>
<section id="dataset-preparation-and-processing">
<h3>Dataset Preparation and Processing<a class="headerlink" href="#dataset-preparation-and-processing" title="Link to this heading">ÔÉÅ</a></h3>
<p>We built a custom dataset loader to convert videos into frames, making them easier to load into memory for training or inference. To enhance our model‚Äôs learning capabilities, we integrated Mediapipe into the dataset loader to extract hand landmarks. However, this integration significantly slowed down the dataset loading process. To address this, we pickled the processed dataset and applied augmentations to the pickled data for training. While this approach worked, it introduced some limitations that could be improved upon, such as exploring alternative landmark extraction models to speed up dataset processing.</p>
<p>Mediapipe also faced challenges in detecting hands consistently, which negatively impacted model performance. To counter this, we implemented interpolation to handle missing landmark values.</p>
<section id="include-subset-50-creation">
<h4>INCLUDE SUBSET 50 Creation<a class="headerlink" href="#include-subset-50-creation" title="Link to this heading">ÔÉÅ</a></h4>
<p>For the INCLUDE SUBSET 50 dataset, we randomly selected words from the larger INCLUDE dataset that had at least 20-21 videos per word. This ensured sufficient samples for training and validation, making the subset manageable while maintaining variability.</p>
</section>
<section id="dataset-loader-versions">
<h4>Dataset Loader Versions<a class="headerlink" href="#dataset-loader-versions" title="Link to this heading">ÔÉÅ</a></h4>
<p>We created two versions of the dataset loader:</p>
<ol class="arabic simple">
<li><p><strong>Landmark-based Tensor Loader</strong>: This version returns a tensor of hand landmarks, suitable for training transformer models. Using this, we achieved an accuracy of 54.55% on the INCLUDE SUBSET 50 (details in the repo). Tuning hyperparameters significantly improved performance, but one challenge with transformers is diagnosing performance bottlenecks. For example, adding interpolation to handle missing Mediapipe detections improved results, highlighting the importance of addressing dataset quality.</p></li>
<li><p><strong>Video Tensor Loader</strong>: This version returns tensors of video frames, making it suitable for CV-based models. This approach significantly improved performance, as discussed in the Model Performance section.</p></li>
</ol>
</section>
<section id="data-augmentation">
<h4>Data Augmentation<a class="headerlink" href="#data-augmentation" title="Link to this heading">ÔÉÅ</a></h4>
<p>Due to the large dataset size and slow speed of Mediapipe, we pickled the processed dataset for efficient use. We then applied the following augmentations:</p>
<ul class="simple">
<li><p><strong>INCLUDE SUBSET 50</strong> (12 frames per video):</p>
<ul>
<li><p>Base dataset with a 30% probability of horizontal flip (used for all augmentations).</p></li>
<li><p>Additional datasets with:</p>
<ul>
<li><p>Random crop.</p></li>
<li><p>Random rotation (-7.5¬∞, 7.5¬∞).</p></li>
<li><p>Combination of random crop and rotation.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Complete INCLUDE Dataset</strong> (24 frames per video):</p>
<ul>
<li><p>Base dataset with no augmentation.</p></li>
<li><p>Additional datasets with:</p>
<ul>
<li><p>Horizontal flip (100% probability).</p></li>
<li><p>Random crop.</p></li>
<li><p>Random rotation (-5¬∞, 5¬∞).</p></li>
<li><p>Combination of horizontal flip (50%) and random rotation (-5¬∞, 5¬∞).</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>We used these augmented datasets combined for final training models on both INCLUDE SUBSET 50 and the complete INCLUDE dataset.</p>
</section>
</section>
<section id="model-exploration">
<h3>Model Exploration<a class="headerlink" href="#model-exploration" title="Link to this heading">ÔÉÅ</a></h3>
<p>We experimented with several models, and r3d_18 consistently performed well across all datasets. While we also explored combinations like ResNet + GRU/LSTM, we observed their potential as lightweight models but could not explore them further due to hardware limitations.</p>
<p>Transformers were also used to explore their capabilities. While they showed promising results (54.55% accuracy on INCLUDE SUBSET 50), we realized that hyperparameter tuning plays a significant role in their performance. However, unlike traditional computer vision tasks where images can be directly visualized to diagnose issues, the inability to easily comprehend the dataset makes it challenging to identify data-related problems.</p>
</section>
<section id="challenges-faced">
<h3>Challenges Faced<a class="headerlink" href="#challenges-faced" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p><strong>Mediapipe Speed</strong>: Mediapipe significantly slowed down the dataset processing. We mitigated this by pickling the processed dataset to avoid repeated computations.</p></li>
<li><p><strong>Hardware Limitations</strong>: We addressed this by using Kaggle or our own devices for testing and rented hardware from Vast AI for intensive training tasks.</p></li>
<li><p><strong>Dataset Loading</strong>: Loading video datasets for training was challenging. We tackled this by creating our custom dataset loader with the help of FFmpeg.</p></li>
</ol>
</section>
<section id="future-improvements">
<h3>Future Improvements<a class="headerlink" href="#future-improvements" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p>Landmark Extraction: Exploring alternatives to Mediapipe or improving its implementation in the dataset loader could enable real-time translation.</p></li>
<li><p>Lightweight Models: ResNet + GRU/LSTM is a promising lightweight model combination that warrants further exploration.</p></li>
<li><p>Enhanced Dataset: Using a more comprehensive dataset with both word-level and sentence-level videos could enable proper translation of sign language sentences into spoken language.</p></li>
<li><p>Hyperparameter Tuning: Further tuning transformer hyperparameters could unlock better performance.</p></li>
</ol>
</section>
</section>
<section id="model-performance">
<h2>Model Performance<a class="headerlink" href="#model-performance" title="Link to this heading">ÔÉÅ</a></h2>
<p>We evaluated our models on both the INCLUDE SUBSET 50 and the complete INCLUDE dataset. Below are the detailed performance metrics:</p>
<section id="include-subset-50">
<h3>INCLUDE SUBSET 50<a class="headerlink" href="#include-subset-50" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p>With Horizontal Flip (30% Probability) only</p>
<ul class="simple">
<li><p>r3d_18: Accuracy: 89.18%</p></li>
<li><p>ResNet18 + GRU: Accuracy: 88.74%</p></li>
</ul>
</li>
<li><p>With All Augmentations</p>
<ul class="simple">
<li><p>r3d_18: Accuracy: 79.44%</p></li>
</ul>
</li>
</ol>
</section>
<section id="complete-include-dataset">
<h3>Complete INCLUDE Dataset<a class="headerlink" href="#complete-include-dataset" title="Link to this heading">ÔÉÅ</a></h3>
<p>We trained the <strong>r3d_18</strong> model and achieved the following metrics:</p>
<p><strong>Accuracy: 89.51%</strong></p>
<p>Micro Metrics:</p>
<ul class="simple">
<li><p>F1 Score: 0.8951</p></li>
<li><p>Precision: 0.8951</p></li>
<li><p>Recall: 0.8951</p></li>
</ul>
<p>Macro Metrics:</p>
<ul class="simple">
<li><p>F1 Score: 0.8895</p></li>
<li><p>Precision: 0.9069</p></li>
<li><p>Recall: 0.8934</p></li>
</ul>
<p>Weighted Metrics:</p>
<ul class="simple">
<li><p>F1 Score: 0.8911</p></li>
<li><p>Precision: 0.9071</p></li>
<li><p>Recall: 0.8951</p></li>
</ul>
</section>
<section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic">
<li><p>Clone the repository:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/Naneet/SignLink-ISL.git
</pre></div>
</div>
</li>
<li><p>Navigate to the project directory</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">SignLink</span><span class="o">-</span><span class="n">ISL</span>
</pre></div>
</div>
</li>
<li><p>Install the required dependencies:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Link to this heading">ÔÉÅ</a></h2>
<p>To understand the flow of the project and replicate the results, follow the notebooks in the recommended order:</p>
<ol class="arabic simple">
<li><p><strong>Video Dataset Conversion</strong></p>
<ul class="simple">
<li><p>Notebook: <code class="docutils literal notranslate"><span class="pre">Video_dataset_conversion_refined.ipynb</span></code></p></li>
<li><p>This notebook demonstrates the process of converting video datasets into frame-based tensors</p></li>
</ul>
</li>
<li><p><strong>Dataset Augmentation</strong></p>
<ul class="simple">
<li><p>Notebook: <code class="docutils literal notranslate"><span class="pre">Dataset_augmentation_pickled.ipynb</span></code></p></li>
<li><p>Apply various augmentations (e.g., horizontal flip, random crop, random rotation) to pickled dataset.</p></li>
</ul>
</li>
<li><p><strong>Training the Models</strong></p>
<ul class="simple">
<li><p>Notebook: <code class="docutils literal notranslate"><span class="pre">Complete_INCLUDE_training_notebook.ipynb</span></code> / <code class="docutils literal notranslate"><span class="pre">Subset_50_training-notebook.ipynb</span></code></p></li>
<li><p>Loads the pickled dataset.</p></li>
<li><p>Covers the training process using different models like r3d_18 and ResNet18 + GRU/LSTM.</p></li>
<li><p>Training configurations (e.g., hyperparameters, loss functions, optimizers).</p></li>
<li><p>Model evaluation metrics and saving trained models.</p></li>
</ul>
</li>
</ol>
<p>üö®üö® We will also be creating docs for it soon for better understanding üö®üö® :)</p>
</section>
<section id="dataset-references">
<h2>Dataset &amp; References<a class="headerlink" href="#dataset-references" title="Link to this heading">ÔÉÅ</a></h2>
<p><a class="reference external" href="https://zenodo.org/records/4010759">INCLUDE Dataset</a></p>
<p><a class="reference internal" href="#/dataset/ISL_SUBSET_50_WORDS.txt"><span class="xref myst">INCLUDE SUBSET 50</span></a></p>
<p><a class="reference external" href="https://dl.acm.org/doi/10.1145/3394171.3413528">INCLUDE: A Large Scale Dataset for Indian Sign Language Recognition</a></p>
<p><a class="reference external" href="https://cse.iitk.ac.in/users/cs365/2015/_submissions/vinsam/report.pdf">Indian Sign Language Character Recognition</a></p>
<p><a class="reference external" href="https://arxiv.org/pdf/1412.2306">Deep Visual-Semantic Alignments for Generating Image Descriptions</a></p>
<p><a class="reference external" href="https://www.irjet.net/archives/V7/i3/IRJET-V7I3418.pdf">Sign language Recognition Using Machine Learning Algorithm</a></p>
<p><a class="reference external" href="https://pytorch.org/docs/stable/index.html">PyTorch Documentation</a></p>
<p><a class="reference internal" href="#/dataset/README.md"><span class="xref myst">Our version of dataset</span></a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="SIGNLINK-ISL documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="utils/index.html" class="btn btn-neutral float-right" title="Utils" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Naneet, Barkknight.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>