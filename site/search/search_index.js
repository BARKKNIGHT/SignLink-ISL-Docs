var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"SignLink-ISL","text":"<p>SignLink-ISL is a deep learning-based system designed to recognize and interpret Indian Sign Language (ISL) gestures, facilitating effective communication between the deaf community and others. This project aims to build a robust Sign Language Recognition (SLR) model, with a specific focus on Indian Sign Language (ISL). The goal is to preprocess video datasets efficiently and train machine learning models to recognize and classify ISL gestures accurately.</p>"},{"location":"index.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Approach</li> <li>Model Performance</li> <li>Installation</li> <li>Usage</li> <li>Dataset &amp; References</li> </ul>"},{"location":"index.html#introduction","title":"Introduction","text":"<p>Effective communication with individuals who are deaf or hard of hearing is increasingly important. To address this need, we developed SignLink-ISL. While the system is currently limited to recognizing and translating isolated sign language words due to dataset constraints, it lays the groundwork for future advancements. We hope this project serves as a starting point for others to build upon, eventually leading to a real-time translation system capable of converting entire sign language sentences into spoken language.</p>"},{"location":"index.html#approach","title":"Approach","text":""},{"location":"index.html#dataset-preparation-and-processing","title":"Dataset Preparation and Processing","text":"<p>We built a custom dataset loader to convert videos into frames, making them easier to load into memory for training or inference. To enhance our model's learning capabilities, we integrated Mediapipe into the dataset loader to extract hand landmarks. However, this integration significantly slowed down the dataset loading process. To address this, we pickled the processed dataset and applied augmentations to the pickled data for training. While this approach worked, it introduced some limitations that could be improved upon, such as exploring alternative landmark extraction models to speed up dataset processing.</p> <p>Mediapipe also faced challenges in detecting hands consistently, which negatively impacted model performance. To counter this, we implemented interpolation to handle missing landmark values.</p>"},{"location":"index.html#include-subset-50-creation","title":"INCLUDE SUBSET 50 Creation","text":"<p>For the INCLUDE SUBSET 50 dataset, we randomly selected words from the larger INCLUDE dataset that had at least 20-21 videos per word. This ensured sufficient samples for training and validation, making the subset manageable while maintaining variability.</p>"},{"location":"index.html#dataset-loader-versions","title":"Dataset Loader Versions","text":"<p>We created two versions of the dataset loader:</p> <ol> <li>Landmark-based Tensor Loader: This version returns a tensor of hand landmarks, suitable for training transformer models. Using this, we achieved an accuracy of 54.55% on the INCLUDE SUBSET 50 (details in the repo). Tuning hyperparameters significantly improved performance, but one challenge with transformers is diagnosing performance bottlenecks. For example, adding interpolation to handle missing Mediapipe detections improved results, highlighting the importance of addressing dataset quality.</li> <li>Video Tensor Loader: This version returns tensors of video frames, making it suitable for CV-based models. This approach significantly improved performance, as discussed in the Model Performance section.</li> </ol>"},{"location":"index.html#data-augmentation","title":"Data Augmentation","text":"<p>Due to the large dataset size and slow speed of Mediapipe, we pickled the processed dataset for efficient use. We then applied the following augmentations:</p> <ul> <li>INCLUDE SUBSET 50 (12 frames per video):</li> <li>Base dataset with a 30% probability of horizontal flip (used for all augmentations).</li> <li>Additional datasets with:<ul> <li>Random crop.</li> <li>Random rotation (-7.5\u00b0, 7.5\u00b0).</li> <li>Combination of random crop and rotation.</li> </ul> </li> <li>Complete INCLUDE Dataset (24 frames per video):</li> <li>Base dataset with no augmentation.</li> <li>Additional datasets with:<ul> <li>Horizontal flip (100% probability).</li> <li>Random crop.</li> <li>Random rotation (-5\u00b0, 5\u00b0).</li> <li>Combination of horizontal flip (50%) and random rotation (-5\u00b0, 5\u00b0).</li> </ul> </li> </ul> <p>We used these augmented datasets combined for final training models on both INCLUDE SUBSET 50 and the complete INCLUDE dataset.</p>"},{"location":"index.html#model-exploration","title":"Model Exploration","text":"<p>We experimented with several models, and r3d_18 consistently performed well across all datasets. While we also explored combinations like ResNet + GRU/LSTM, we observed their potential as lightweight models but could not explore them further due to hardware limitations.</p> <p>Transformers were also used to explore their capabilities. While they showed promising results (54.55% accuracy on INCLUDE SUBSET 50), we realized that hyperparameter tuning plays a significant role in their performance. However, unlike traditional computer vision tasks where images can be directly visualized to diagnose issues, the inability to easily comprehend the dataset makes it challenging to identify data-related problems.</p>"},{"location":"index.html#challenges-faced","title":"Challenges Faced","text":"<ol> <li>Mediapipe Speed: Mediapipe significantly slowed down the dataset processing. We mitigated this by pickling the processed dataset to avoid repeated computations.</li> <li>Hardware Limitations: We addressed this by using Kaggle or our own devices for testing and rented hardware from Vast AI for intensive training tasks.</li> <li>Dataset Loading: Loading video datasets for training was challenging. We tackled this by creating our custom dataset loader with the help of FFmpeg.</li> </ol>"},{"location":"index.html#future-improvements","title":"Future Improvements","text":"<ol> <li>Landmark Extraction: Exploring alternatives to Mediapipe or improving its implementation in the dataset loader could enable real-time translation.</li> <li>Lightweight Models: ResNet + GRU/LSTM is a promising lightweight model combination that warrants further exploration.</li> <li>Enhanced Dataset: Using a more comprehensive dataset with both word-level and sentence-level videos could enable proper translation of sign language sentences into spoken language.</li> <li>Hyperparameter Tuning: Further tuning transformer hyperparameters could unlock better performance.</li> </ol>"},{"location":"index.html#model-performance","title":"Model Performance","text":"<p>We evaluated our models on both the INCLUDE SUBSET 50 and the complete INCLUDE dataset. Below are the detailed performance metrics:</p>"},{"location":"index.html#include-subset-50","title":"INCLUDE SUBSET 50","text":"<ol> <li>With Horizontal Flip (30% Probability) only</li> <li>r3d_18: Accuracy: 89.18%</li> <li>ResNet18 + GRU: Accuracy: 88.74%</li> <li>With All Augmentations</li> <li>r3d_18: Accuracy: 79.44%</li> </ol>"},{"location":"index.html#complete-include-dataset","title":"Complete INCLUDE Dataset","text":"<p>We trained the r3d_18 model and achieved the following metrics:</p> <p>Accuracy: 89.51%</p> <p>Micro Metrics:  - F1 Score: 0.8951  - Precision: 0.8951  - Recall: 0.8951</p> <p>Macro Metrics:  - F1 Score: 0.8895  - Precision: 0.9069  - Recall: 0.8934</p> <p>Weighted Metrics:  - F1 Score: 0.8911  - Precision: 0.9071  - Recall: 0.8951</p>"},{"location":"404.html","title":"404 - Page Not Found","text":"<p>\ud83d\ude15 Oops! It seems you've wandered off the beaten path.</p> <p>The page you\u2019re looking for doesn\u2019t exist. This could be because:</p> <ul> <li>The URL was mistyped.</li> <li>The page has been moved or deleted.</li> <li>You\u2019ve discovered a broken link (uh-oh!).</li> </ul>"},{"location":"404.html#what-can-you-do","title":"What can you do?","text":"<ul> <li>Return to the Home Page.</li> <li>Search the Documentation to find what you need.</li> <li>Double-check the URL for any errors.</li> </ul> <p>If you believe this is an issue, feel free to report it on GitHub. We appreciate your help! \ud83d\udca1</p>"},{"location":"about.html","title":"SignLink-ISL","text":"<p>SignLink-ISL is a deep learning-based system designed to recognize and interpret Indian Sign Language (ISL) gestures, facilitating effective communication between the deaf community and others. This project aims to build a robust Sign Language Recognition (SLR) model, with a specific focus on Indian Sign Language (ISL). The goal is to preprocess video datasets efficiently and train machine learning models to recognize and classify ISL gestures accurately.</p>"},{"location":"dataset.html","title":"Dataset &amp; References","text":"<p>INCLUDE Dataset</p> <p>INCLUDE SUBSET 50</p> <p>INCLUDE: A Large Scale Dataset for Indian Sign Language Recognition</p> <p>Indian Sign Language Character Recognition</p> <p>Deep Visual-Semantic Alignments for Generating Image Descriptions</p> <p>Sign language Recognition Using Machine Learning Algorithm</p> <p>PyTorch Documentation</p> <p>Our version of dataset</p>"},{"location":"setup.html","title":"Installation","text":""},{"location":"setup.html#commands","title":"Commands","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/Naneet/SignLink-ISL.git\n</code></pre></p> </li> <li> <p>Navigate to the project directory    <pre><code>cd SignLink-ISL\n</code></pre></p> </li> <li> <p>Install the required dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"setup.html#usage","title":"Usage","text":"<p>To understand the flow of the project and replicate the results, follow the notebooks in the recommended order:</p> <ol> <li>Video Dataset Conversion</li> <li>Notebook: <code>Video_dataset_conversion_refined.ipynb</code></li> <li>This notebook demonstrates the process of converting video datasets into frame-based tensors</li> <li>Dataset Augmentation</li> <li>Notebook: <code>Dataset_augmentation_pickled.ipynb</code></li> <li>Apply various augmentations (e.g., horizontal flip, random crop, random rotation) to pickled dataset.</li> <li>Training the Models</li> <li>Notebook: <code>Complete_INCLUDE_training_notebook.ipynb</code> / <code>Subset_50_training-notebook.ipynb</code></li> <li>Loads the pickled dataset.</li> <li>Covers the training process using different models like r3d_18 and ResNet18 + GRU/LSTM.</li> <li>Training configurations (e.g., hyperparameters, loss functions, optimizers).</li> <li>Model evaluation metrics and saving trained models.</li> </ol> <p>\ud83d\udea8\ud83d\udea8 We will also be creating docs for it soon for better understanding \ud83d\udea8\ud83d\udea8 :)</p>"},{"location":"training.html","title":"Training procedure :","text":"<pre><code>python train.py\n</code></pre>"},{"location":"trainers/index.html","title":"Utils","text":"<p>This section contains classes necessary for training models using dataloaders or pickled data.</p>"},{"location":"trainers/index.html#contents","title":"Contents","text":""},{"location":"trainers/index.html#trainer-class","title":"Trainer Class","text":"<p>Class for training with <code>torch.utils.data.DataLoader</code> dataset loader.</p>"},{"location":"trainers/index.html#trainer_batch_slice-class","title":"Trainer_Batch_Slice Class","text":"<p>Class for training with <code>pickled_data</code> dataset and enables adaptive slicing of batch tensors into equal parts for training in <code>vram</code> constrained environments.</p>"},{"location":"trainers/index.html#trainer_pickle-class","title":"Trainer_Pickle Class","text":"<p>Class for training with  <code>pickled_data</code> dataset.</p>"},{"location":"trainers/index.html#trainer_pickle_preload-class","title":"Trainer_Pickle_Preload Class","text":"<p>Class for training with  <code>pickled_data</code> dataset and invloves tools like <code>utils.pickle.preload_tensors(path_list, device)</code> to prefetch data into vram to improve performance by reducing <code>I/O</code> fetching between epochs. \u26a0\ufe0f\u26a0\ufe0f Only for Environments where the entire dataset fits the vram constraints\u26a0\ufe0f\u26a0\ufe0f.</p>"},{"location":"trainers/trainer/init.html","title":"<code>__init__</code>","text":""},{"location":"trainers/trainer/init.html#initializes-an-object-of-trainer-class-with-necessary-parameters-to-train-a-model","title":"Initializes an object of Trainer Class with necessary parameters to train a model.","text":""},{"location":"trainers/trainer/init.html#function-definition","title":"Function Definition","text":"<pre><code>__init__(self,model,device,optimizer,scheduler,loss_fn,train_dataloader,test_dataloader,save=False)\n</code></pre>"},{"location":"trainers/trainer/init.html#parameters","title":"Parameters","text":""},{"location":"trainers/trainer/init.html#model","title":"<code>model</code>","text":"<ul> <li>Type: <code>torch.nn.Module</code></li> <li>Description: The neural network model to be trained and evaluated.</li> </ul>"},{"location":"trainers/trainer/init.html#device","title":"<code>device</code>","text":"<ul> <li>Type: <code>torch.device</code></li> <li>Description: Specifies the computation device (<code>'cpu'</code> or <code>'cuda'</code>).</li> </ul>"},{"location":"trainers/trainer/init.html#optimizer","title":"<code>optimizer</code>","text":"<ul> <li>Type: <code>torch.optim.Optimizer</code></li> <li>Description: Optimizer used for updating model weights during training.</li> </ul>"},{"location":"trainers/trainer/init.html#scheduler","title":"<code>scheduler</code>","text":"<ul> <li>Type: <code>torch.optim.lr_scheduler._LRScheduler</code></li> <li>Description: Learning rate scheduler to adjust the learning rate during training.</li> </ul>"},{"location":"trainers/trainer/init.html#loss_fn","title":"<code>loss_fn</code>","text":"<ul> <li>Type: <code>callable</code></li> <li>Description: The loss function to compute the difference between predictions and target labels.</li> </ul>"},{"location":"trainers/trainer/init.html#train_dataloader","title":"<code>train_dataloader</code>","text":"<ul> <li>Type: <code>torch.utils.data.DataLoader</code></li> <li>Description: DataLoader for the training dataset.</li> </ul>"},{"location":"trainers/trainer/init.html#test_dataloader","title":"<code>test_dataloader</code>","text":"<ul> <li>Type: <code>torch.utils.data.DataLoader</code></li> <li>Description: DataLoader for the testing/validation dataset.</li> </ul>"},{"location":"trainers/trainer/init.html#save","title":"<code>save</code>","text":"<ul> <li>Type: <code>bool</code></li> <li>Default: <code>False</code></li> <li>Description: Flag indicating whether to save the model after training.</li> </ul>"},{"location":"trainers/trainer/init.html#example-usage","title":"Example Usage","text":"<pre><code>from torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.cuda.amp import GradScaler\nfrom trainers.trainer import Trainer\nfrom models.Conv3D import SignLanguageClassifier\n\n# Define components\nmodel = SignLanguageClassifier(len(word_to_idx)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nloss_fn = nn.CrossEntropyLoss()\n\nscheduler = ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.1,\n    patience=5,\n    min_lr=1e-6\n)\n\n# Initialize\ntrainer = Trainer(\n    model=model,\n    device=device,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loss_fn=loss_fn,\n    train_dataloader=train_dataloader,\n    test_dataloader=test_dataloader,\n    save=True\n)\n</code></pre>"},{"location":"trainers/trainer/train_step.html","title":"<code>train_step</code>","text":""},{"location":"trainers/trainer/train_step.html#purpose","title":"Purpose","text":"<p>Executes a single training epoch for the model using the training dataset. It utilizes automatic mixed precision (AMP) for efficient computation on CUDA-enabled devices. The method computes loss, updates model weights, and tracks training accuracy.</p>"},{"location":"trainers/trainer/train_step.html#parameters","title":"Parameters","text":""},{"location":"trainers/trainer/train_step.html#epoch","title":"<code>epoch</code>","text":"<ul> <li>Type: <code>int</code></li> <li>Description: The current epoch number, used for logging and model saving purposes.</li> </ul>"},{"location":"trainers/trainer/train_step.html#workflow","title":"Workflow","text":"<ol> <li> <p>Set Model to Training Mode: <code>self.model.train()</code> sets the model in training mode to enable gradient computation.</p> </li> <li> <p>Initialize Metrics:  </p> </li> <li><code>train_loss</code>: Tracks cumulative loss across all batches in the epoch.  </li> <li><code>total_correct</code>: Tracks the number of correct predictions.  </li> <li> <p><code>total_samples</code>: Tracks the total number of samples processed.</p> </li> <li> <p>Iterate Through Training DataLoader:</p> </li> <li>Moves input (<code>X</code>) and labels (<code>y</code>) to the specified computation device.</li> <li>Performs forward pass using AMP:<ul> <li>Computes model predictions (<code>y_pred</code>).</li> <li>Calculates loss using <code>self.loss_fn</code>.</li> </ul> </li> <li>Scales the loss for AMP and performs backpropagation.</li> <li>Updates model parameters using <code>self.optimizer</code>.</li> <li> <p>Resets gradients after each update.</p> </li> <li> <p>Compute Accuracy:</p> </li> <li>Converts predictions to class labels.</li> <li>Compares predictions with ground truth labels to compute batch accuracy.</li> <li> <p>Aggregates results for all batches.</p> </li> <li> <p>Log Metrics:</p> </li> <li> <p>Computes and prints epoch-level loss and accuracy.</p> </li> <li> <p>Adjust Learning Rate (if using <code>ReduceLROnPlateau</code> scheduler):</p> </li> <li>Calls <code>self.scheduler.step()</code> with the epoch loss.</li> </ol>"},{"location":"trainers/trainer/train_step.html#key-components","title":"Key Components","text":""},{"location":"trainers/trainer/train_step.html#automatic-mixed-precision-amp","title":"Automatic Mixed Precision (AMP)","text":"<ul> <li>Purpose: Improves training speed and reduces memory usage by using lower precision (FP16) during computations.</li> <li>Implementation: <code>autocast('cuda')</code> and <code>GradScaler</code>.</li> </ul>"},{"location":"trainers/trainer/train_step.html#accuracy-calculation","title":"Accuracy Calculation","text":"<ul> <li>Formula:   [   \\text{Accuracy} = \\left( \\frac{\\text{Total Correct Predictions}}{\\text{Total Samples}} \\right) \\times 100   ]</li> </ul>"},{"location":"trainers/trainer/train_step.html#example-usage","title":"Example Usage","text":"<pre><code># Train the model for one epoch\nmy_object.train_step(epoch=1)\n</code></pre>"},{"location":"trainers/trainer/train_step.html#output","title":"Output","text":"<p>Logs the following metrics to the console: - Epoch number - Average training loss for the epoch - Training accuracy (percentage)</p> <p>Example Output: <pre><code>Epoch: 1 | Train Loss: 0.2458 | Accuracy: 89.52\n</code></pre></p>"},{"location":"utils/index.html","title":"Utils","text":"<p>This section contains utility modules and documentation for various functionalities.</p>"},{"location":"utils/index.html#contents","title":"Contents","text":""},{"location":"utils/index.html#data-and-words-utilities","title":"Data and Words Utilities","text":"<p>Tools and utilities for processing data and word-related functionalities.</p>"},{"location":"utils/index.html#metric-utilities","title":"Metric Utilities","text":"<p>Tools and utilities for displaying and computing necessary metrics for evaluating the models.</p>"},{"location":"utils/index.html#pickle-utilities","title":"Pickle Utilities","text":"<p>Tools and utilities for working with pickle files, including data storage and path management.</p>"},{"location":"utils/index.html#preload-utilities","title":"Preload Utilities","text":"<p>Tools and utilities for preloading tensors into the selected device.</p>"},{"location":"utils/index.html#save-and-load-utilities","title":"Save and Load Utilities","text":"<p>Tools and utilities for saving and loading models from <code>.pt</code> files.</p>"},{"location":"utils/index.html#show-sequence-utilities","title":"Show Sequence Utilities","text":"<p>Tools and utilities for displaying video tensors.</p>"},{"location":"utils/data_words/data_and_words.html","title":"<code>data_and_words</code>","text":""},{"location":"utils/data_words/data_and_words.html#overview","title":"Overview","text":"<p>The <code>data_and_words</code> function loads file paths and their corresponding labels from a dataset directory structure. Each subdirectory name is treated as a label, and the files within represent the data samples for that label.</p>"},{"location":"utils/data_words/data_and_words.html#function-definition","title":"Function Definition","text":"<pre><code>utils.data_words.data_and_words(path)\n</code></pre>"},{"location":"utils/data_words/data_and_words.html#arguments","title":"Arguments","text":"<ul> <li>path (<code>str</code>):</li> <li>Path to the root directory of the dataset. Each subdirectory should represent a label and contain files belonging to that category.</li> </ul>"},{"location":"utils/data_words/data_and_words.html#returns","title":"Returns","text":"<ul> <li>tuple: A tuple containing:</li> <li>data (<code>list</code>):<ul> <li>A list of tuples where each tuple consists of:</li> <li>The file path (<code>str</code>) to a data sample (e.g., video).</li> <li>The numeric label (<code>int</code>) corresponding to the category.</li> </ul> </li> <li>words_list (<code>list</code>):<ul> <li>A list of category names (<code>str</code>) corresponding to their numeric labels.</li> </ul> </li> <li>words_dict (<code>dict</code>):<ul> <li>A dictionary mapping category names (<code>str</code>) to their numeric labels (<code>int</code>).</li> </ul> </li> </ul>"},{"location":"utils/data_words/data_and_words.html#notes","title":"Notes","text":"<ul> <li>The numeric labels are assigned in the order of the subdirectories as returned by <code>os.listdir</code>.</li> <li>Ensure that the directory structure is consistent, with subdirectories representing categories and containing relevant files.</li> </ul>"},{"location":"utils/data_words/data_and_words.html#example","title":"Example","text":"<pre><code>Directory structure:\nroot/\n\u251c\u2500\u2500 cat/\n\u2502   \u251c\u2500\u2500 cat1.mp4\n\u2502   \u251c\u2500\u2500 cat2.mp4\n\u251c\u2500\u2500 dog/\n\u2502   \u251c\u2500\u2500 dog1.mp4\n</code></pre>"},{"location":"utils/data_words/data_and_words.html#function-call","title":"Function call:","text":"<pre><code>data, words_list, words_dict = data_and_words(\"root\")\n</code></pre>"},{"location":"utils/data_words/data_and_words.html#output","title":"Output:","text":"<pre><code>data: [(\"root/cat/cat1.mp4\", 0), (\"root/cat/cat2.mp4\", 0), (\"root/dog/dog1.mp4\", 1)]\nwords_list: [\"cat\", \"dog\"]\nwords_dict: {\"cat\": 0, \"dog\": 1}\n</code></pre>"},{"location":"utils/data_words/data_and_words.html#implementation","title":"Implementation","text":"<pre><code>def data_and_words(path):\n    \"\"\"\n    Loads file paths and their corresponding labels from a dataset directory structure.\n\n    Args:\n        path (str): Path to the root directory of the dataset. \n                    Each subdirectory should represent a label and contain files belonging to that category.\n\n    Returns:\n        tuple: A tuple containing:\n            - data (list): A list of tuples where each tuple consists of:\n                - The file path (str) to a data sample (e.g., video).\n                - The numeric label (int) corresponding to the category.\n            - words_list (list): A list of category names (str) corresponding to their numeric labels.\n            - words_dict (dict): A dictionary mapping category names (str) to their numeric labels (int).\n\n    Notes:\n        - The numeric labels are assigned in the order of the subdirectories as returned by `os.listdir`.\n        - Ensure that the directory structure is consistent, with subdirectories representing categories\n          and containing relevant files.\n\n    Example:\n        If the directory structure is:\n        root/\n        \u251c\u2500\u2500 cat/\n        \u2502   \u251c\u2500\u2500 cat1.mp4\n        \u2502   \u251c\u2500\u2500 cat2.mp4\n        \u251c\u2500\u2500 dog/\n        \u2502   \u251c\u2500\u2500 dog1.mp4\n\n        Calling `data_and_words(\"root\")` will return:\n            - data: [(\"root/cat/cat1.mp4\", 0), (\"root/cat/cat2.mp4\", 0), (\"root/dog/dog1.mp4\", 1)]\n            - words_list: [\"cat\", \"dog\"]\n            - words_dict: {\"cat\": 0, \"dog\": 1}\n    \"\"\"\n    words = os.listdir(path)\n    words_list = []\n    data = []\n    for word in words:\n        words_list.append(word)\n        word_videos_path = os.path.join(path, word)\n        videos = os.listdir(word_videos_path)\n        for video in videos:\n            data.append((os.path.join(word_videos_path, video), len(words_list)-1))\n\n    words_dict = dict({})\n    for num, word in enumerate(words_list):\n        words_dict[word] = num\n\n    return data, words_list, words_dict\n</code></pre>"},{"location":"utils/data_words/random_split.html","title":"<code>random_split</code>","text":""},{"location":"utils/data_words/random_split.html#overview","title":"Overview","text":"<p>The <code>random_split</code> function is used to split a dataset into training and testing sets based on a specified ratio. It ensures that the split is random and reproducible using a fixed seed.</p>"},{"location":"utils/data_words/random_split.html#function-definition","title":"Function Definition","text":"<pre><code>utils.data_words.random_split(data_dict, train_ratio=0.8, seed=42)\n</code></pre>"},{"location":"utils/data_words/random_split.html#arguments","title":"Arguments","text":"<ul> <li>data_dict (<code>dict</code>):</li> <li> <p>A dictionary where keys are labels (e.g., strings) and values are lists of paths (e.g., file paths or data samples corresponding to those labels).</p> </li> <li> <p>train_ratio (<code>float</code>, optional):</p> </li> <li> <p>The proportion of data to include in the training set. Defaults to <code>0.8</code>.</p> </li> <li> <p>seed (<code>int</code>, optional):</p> </li> <li>Random seed for reproducibility of the data split. Defaults to <code>42</code>.</li> </ul>"},{"location":"utils/data_words/random_split.html#returns","title":"Returns","text":"<ul> <li>tuple: Returns two lists, <code>train_data</code> and <code>test_data</code>.</li> <li>train_data (<code>list</code>):<ul> <li>A list of tuples, where each tuple contains a path and its corresponding label index, e.g., <code>[(path1, label_index1), (path2, label_index2), ...]</code>.</li> </ul> </li> <li>test_data (<code>list</code>):<ul> <li>A list of tuples, similar to <code>train_data</code>, but for the testing set.</li> </ul> </li> </ul>"},{"location":"utils/data_words/random_split.html#notes","title":"Notes","text":"<ul> <li>Paths for each label are shuffled before splitting to ensure randomness.</li> </ul>"},{"location":"utils/data_words/random_split.html#example","title":"Example","text":"<pre><code>data_dict = {\n    \"cat\": [\"cat1.jpg\", \"cat2.jpg\", \"cat3.jpg\"],\n    \"dog\": [\"dog1.jpg\", \"dog2.jpg\"]\n}\nword_to_idx = {\"cat\": 0, \"dog\": 1}\n\ntrain_data, test_data = random_split(data_dict, train_ratio=0.7)\n</code></pre>"},{"location":"utils/data_words/random_split.html#output","title":"Output","text":"<pre><code>train_data: [(\"cat1.jpg\", 0), (\"dog1.jpg\", 1), ...]\ntest_data: [(\"cat3.jpg\", 0), (\"dog2.jpg\", 1)]\n</code></pre>"},{"location":"utils/data_words/random_split.html#implementation","title":"Implementation","text":"<pre><code>def random_split(data_dict, word_to_idx, train_ratio=0.8, seed=42):\n    \"\"\"\n    Splits a dataset into training and testing sets based on the specified ratio.\n\n    Args:\n        data_dict (dict): A dictionary where keys are labels (e.g., strings), and values are lists of paths \n                          (e.g., file paths or data samples corresponding to those labels).\n        train_ratio (float, optional): The proportion of data to include in the training set. Defaults to 0.8.\n        seed (int, optional): Random seed for reproducibility of the data split. Defaults to 42.\n\n    Returns:\n        tuple: Two lists, `train_data` and `test_data`.\n            - train_data: A list of tuples, where each tuple contains a path and its corresponding label index, \n                          e.g., [(path1, label_index1), (path2, label_index2), ...].\n            - test_data: A list of tuples, similar to `train_data`, but for the testing set.\n\n    Notes:\n        - This function assumes a global or pre-defined dictionary `word_to_idx` mapping label names to integer indices.\n        - Paths for each label are shuffled before splitting to ensure randomness.\n\n    Example:\n        data_dict = {\n            \"cat\": [\"cat1.jpg\", \"cat2.jpg\", \"cat3.jpg\"],\n            \"dog\": [\"dog1.jpg\", \"dog2.jpg\"]\n        }\n        word_to_idx = {\"cat\": 0, \"dog\": 1}\n\n        train_data, test_data = random_split(data_dict, train_ratio=0.7)\n        # train_data: [(\"cat1.jpg\", 0), (\"dog1.jpg\", 1), ...]\n        # test_data: [(\"cat3.jpg\", 0), (\"dog2.jpg\", 1)]\n    \"\"\"\n    np.random.seed(seed)\n    train_data = []\n    test_data = []\n\n    for label, paths in data_dict.items():\n        # Shuffle the paths\n        paths = np.array(paths)\n        np.random.shuffle(paths)\n\n        # Split into training and testing\n        split_idx = int(len(paths) * train_ratio)\n        train_paths = paths[:split_idx].tolist()\n        test_paths = paths[split_idx:].tolist()\n\n        # Append the paths with their labels\n        train_data.extend([(path, word_to_idx[label]) for path in train_paths])\n        test_data.extend([(path, word_to_idx[label]) for path in test_paths])\n\n    return train_data, test_data\n</code></pre>"},{"location":"utils/metrics/display_conf_matrix.html","title":"<code>display_conf_matrix</code>","text":"<p>Displays the confusion matrix of predicted and true labels.</p>"},{"location":"utils/metrics/display_conf_matrix.html#description","title":"Description","text":"<p>This function calculates and visualizes a confusion matrix for the given true and predicted labels. The matrix is plotted using a customizable colormap for better readability. The confusion matrix is returned as a NumPy array, which can be used for further analysis or inference.</p>"},{"location":"utils/metrics/display_conf_matrix.html#function-definition","title":"Function Definition","text":"<pre><code>utils.metrics.display_conf_matrix(y_true, y_pred, num_words=50)\n</code></pre>"},{"location":"utils/metrics/display_conf_matrix.html#arguments","title":"Arguments","text":"<ul> <li> <p><code>y_true</code> (<code>array-like</code>):   The true labels for the dataset.</p> </li> <li> <p><code>y_pred</code> (<code>array-like</code>):   The predicted labels for the dataset.</p> </li> <li> <p><code>num_words</code> (<code>int</code>, optional):   The number of classes to be displayed in the confusion matrix. Default is <code>50</code>. This argument is useful when the number of classes is large, and the matrix might become too cluttered.</p> </li> </ul>"},{"location":"utils/metrics/display_conf_matrix.html#example-usage","title":"Example Usage","text":"<pre><code>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Example true and predicted labels\ny_true = [0, 1, 2, 2, 0, 1]\ny_pred = [0, 1, 2, 1, 0, 0]\n\n# Display the confusion matrix\ncm = display_conf_matrix(y_true, y_pred)\n</code></pre>"},{"location":"utils/metrics/display_conf_matrix.html#returned-value","title":"Returned Value","text":"<ul> <li><code>cm</code> (<code>ndarray</code>):   A NumPy array containing the confusion matrix. This array can be used for further analysis or processing.</li> </ul>"},{"location":"utils/metrics/display_conf_matrix.html#notes","title":"Notes","text":"<ul> <li>The function uses <code>ConfusionMatrixDisplay</code> from <code>sklearn</code> to visualize the confusion matrix.</li> <li>The matrix is displayed using the <code>viridis</code> colormap, but this can be customized.</li> <li>The <code>num_words</code> argument ensures that the confusion matrix can handle a large number of classes.</li> </ul>"},{"location":"utils/metrics/display_conf_matrix.html#implementation","title":"Implementation","text":"<pre><code>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef display_conf_matrix(y_true, y_pred, num_words=50):\n    \"\"\"\n    Displays and returns the confusion matrix for the given true and predicted labels.\n\n    Args:\n        y_true (array-like): The true labels for the dataset.\n        y_pred (array-like): The predicted labels for the dataset.\n        num_words (int, optional): The number of classes to display in the matrix. Default is 50.\n\n    Returns:\n        numpy.ndarray: The confusion matrix as a NumPy array.\n\n    Example:\n        cm = display_conf_matrix(y_true, y_pred)\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_words))\n    disp.plot(cmap=\"viridis\")  # Customize colormap if needed\n    return cm  # Returning cm because there are too many classes and the Confusion Matrix gets messy.\n</code></pre>"},{"location":"utils/metrics/show_performance_scores.html","title":"<code>show_performance_scores</code>","text":"<p>Displays performance metrics (F1 score, precision, recall) for multiple averaging methods (micro, macro, weighted) and overall accuracy.</p>"},{"location":"utils/metrics/show_performance_scores.html#description","title":"Description","text":"<p>This function calculates and prints performance scores for classification tasks based on three averaging methods: 'micro', 'macro', and 'weighted'. It evaluates F1 score, precision, and recall for each averaging method and also computes the overall accuracy.</p>"},{"location":"utils/metrics/show_performance_scores.html#function-definition","title":"Function Definition","text":"<pre><code>utils.metrics.show_performance_scores(y_true, y_pred)\n</code></pre>"},{"location":"utils/metrics/show_performance_scores.html#arguments","title":"Arguments","text":"<ul> <li> <p><code>y_true</code> (<code>array-like</code>):   The true labels for the dataset.</p> </li> <li> <p><code>y_pred</code> (<code>array-like</code>):   The predicted labels for the dataset.</p> </li> </ul>"},{"location":"utils/metrics/show_performance_scores.html#example-usage","title":"Example Usage","text":"<pre><code>from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Example true and predicted labels\ny_true = [0, 1, 2, 2, 0, 1]\ny_pred = [0, 1, 2, 1, 0, 0]\n\n# Show performance scores\nshow_performance_scores(y_true, y_pred)\n</code></pre>"},{"location":"utils/metrics/show_performance_scores.html#printed-output","title":"Printed Output","text":"<p>The function will print the following for each averaging type (micro, macro, weighted):</p> <ul> <li>F1 score</li> <li>Precision</li> <li>Recall</li> </ul> <p>It will also print the overall accuracy at the end.</p>"},{"location":"utils/metrics/show_performance_scores.html#notes","title":"Notes","text":"<ul> <li>The function supports multiple averaging methods: <code>micro</code>, <code>macro</code>, and <code>weighted</code>. </li> <li>Micro: Calculates metrics globally by counting the total true positives, false positives, etc.</li> <li>Macro: Calculates metrics for each label, then finds their unweighted mean.</li> <li>Weighted: Calculates metrics for each label, and finds their average weighted by support (the number of true instances for each label).</li> <li>The accuracy score is calculated using <code>accuracy_score</code> from <code>sklearn.metrics</code>.</li> </ul>"},{"location":"utils/metrics/show_performance_scores.html#implementation","title":"Implementation","text":"<pre><code>from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\ndef show_performance_scores(y_true, y_pred):\n    \"\"\"\n    Displays performance metrics (F1 score, precision, recall) for multiple averaging methods and overall accuracy.\n\n    Args:\n        y_true (array-like): The true labels for the dataset.\n        y_pred (array-like): The predicted labels for the dataset.\n\n    Example:\n        show_performance_scores(y_true, y_pred)\n    \"\"\"\n    types = ['micro', 'macro', 'weighted']\n    for ty in types:\n        print(ty)\n        precision = precision_score(y_true, y_pred, average=ty) \n        recall = recall_score(y_true, y_pred, average=ty)        \n        f1 = f1_score(y_true, y_pred, average=ty) \n        print(f\"F1 Score: {f1:.4f}\")\n        print(f\"Precision: {precision:.4f}\")\n        print(f\"Recall: {recall:.4f}\")\n        print(\"***************\")\n    print(f\"Acc = {accuracy_score(y_true, y_pred):.4f}\")\n</code></pre>"},{"location":"utils/pickle/get_all_file_paths.html","title":"<code>get_all_file_paths</code>","text":"<p>Recursively collects all file paths in a specified directory.</p>"},{"location":"utils/pickle/get_all_file_paths.html#description","title":"Description","text":"<p>This function traverses a directory and its subdirectories to collect the paths of all files within. It is useful for tasks that require batch processing of files, such as loading datasets or organizing input files for machine learning workflows.</p>"},{"location":"utils/pickle/get_all_file_paths.html#function-definition","title":"Function Definition","text":"<pre><code>utils.pickle.get_all_file_paths(path) -&gt; list\n</code></pre>"},{"location":"utils/pickle/get_all_file_paths.html#arguments","title":"Arguments","text":"<ul> <li><code>path</code> (<code>str</code>):   The root directory to start searching for files.</li> </ul>"},{"location":"utils/pickle/get_all_file_paths.html#behavior","title":"Behavior","text":"<p>The function walks through the directory tree using the <code>os.walk</code> method, which yields the directory path, subdirectories, and filenames at each level. It appends the full file paths (combination of directory path and filenames) to a list, which is returned as the result.</p>"},{"location":"utils/pickle/get_all_file_paths.html#detailed-steps","title":"Detailed Steps","text":"<ol> <li>Initialize an empty list <code>path_list</code>.</li> <li>Use <code>os.walk</code> to iterate through each directory and subdirectory starting from the provided <code>path</code>.</li> <li>For each file found, construct its full path using <code>os.path.join</code> and append it to <code>path_list</code>.</li> <li>Return the <code>path_list</code> containing all file paths.</li> </ol>"},{"location":"utils/pickle/get_all_file_paths.html#example-usage","title":"Example Usage","text":"<pre><code># Example usage\nfile_paths = get_all_file_paths('/path/to/directory')\nprint(file_paths)\n</code></pre> <p>This code will print a list of all file paths within <code>/path/to/directory</code> and its subdirectories.</p>"},{"location":"utils/pickle/get_all_file_paths.html#notes","title":"Notes","text":"<ul> <li>Ensure the provided <code>path</code> is valid and accessible.</li> <li>The function will return an empty list if the directory does not contain any files.</li> <li>This function does not perform any filtering based on file types or extensions; if filtering is needed, you can modify the code to include a condition, e.g., <code>if file.endswith('.pt'):</code>.</li> </ul>"},{"location":"utils/pickle/get_all_file_paths.html#implementation","title":"Implementation","text":"<pre><code>import os\n\ndef get_all_file_paths(path) -&gt; list:\n    \"\"\"\n    Recursively collects all file paths in a specified directory.\n\n    Args:\n        path (str): The root directory to start searching for files.\n\n    Returns:\n        list: A list of file paths (str) for all files in the directory and its subdirectories.\n    \"\"\"\n    path_list = []\n    for top, dirs, files in os.walk(path):\n        for file in files:       \n            path_list.append(os.path.join(top, file))\n    return path_list\n</code></pre>"},{"location":"utils/pickle/pickle_data.html","title":"<code>pickle_data</code>","text":"<p>Save a single data sample (video tensor and label tensor) to disk as a <code>.pt</code> file.</p>"},{"location":"utils/pickle/pickle_data.html#description","title":"Description","text":"<p>The function saves a video tensor and its corresponding label tensor to a specified directory as a <code>.pt</code> file. The file will be named using a specified index to ensure uniqueness.</p>"},{"location":"utils/pickle/pickle_data.html#function-definition","title":"Function Definition","text":"<pre><code>utils.pickle.pickle_data(X, y, n, path)\n</code></pre>"},{"location":"utils/pickle/pickle_data.html#arguments","title":"Arguments","text":"<ul> <li> <p><code>X</code> (<code>torch.Tensor</code>):   The video tensor to be saved.</p> </li> <li> <p><code>y</code> (<code>torch.Tensor</code>):   The label tensor corresponding to the video.</p> </li> <li> <p><code>n</code> (<code>int</code>):   The index used to name the saved file. The file will be named <code>tensor_&lt;n&gt;.pt</code>.</p> </li> <li> <p><code>path</code> (<code>str</code>):   The directory where the tensor file will be saved. If the directory does not exist, it will be created automatically.</p> </li> </ul>"},{"location":"utils/pickle/pickle_data.html#behavior","title":"Behavior","text":"<ul> <li>A dictionary containing the video tensor and label tensor is created and saved to disk as a <code>.pt</code> file.</li> <li>The file is named <code>tensor_&lt;n&gt;.pt</code>, where <code>&lt;n&gt;</code> corresponds to the provided index.</li> <li>If the specified directory does not exist, it is created automatically.</li> </ul>"},{"location":"utils/pickle/pickle_data.html#example-usage","title":"Example Usage","text":"<pre><code>import torch\nimport os\n\n# Example tensors\nX = torch.randn(3, 224, 224)  # Example video tensor\ny = torch.tensor(1)          # Example label tensor\n\n# Save the data\npickle_data(X, y, 0, \"./output_tensors\")\n</code></pre>"},{"location":"utils/pickle/pickle_data.html#saved-file-structure","title":"Saved File Structure","text":"<p>For a <code>path</code> of <code>./output_tensors</code> and an index <code>n = 0</code>: <pre><code>./output_tensors/\n    tensor_0.pt\n</code></pre></p> <p>The <code>.pt</code> file contains a dictionary with two keys: - <code>video</code>: The video tensor. - <code>label</code>: The label tensor.</p>"},{"location":"utils/pickle/pickle_data.html#notes","title":"Notes","text":"<ul> <li>The <code>os.makedirs</code> function ensures the directory exists or is created automatically.</li> <li>Files are saved using <code>torch.save</code>, which serializes the tensors to the <code>.pt</code> format.</li> </ul>"},{"location":"utils/pickle/pickle_data.html#implementation","title":"Implementation","text":"<pre><code>def pickle_data(X : torch.tensor, y : torch.tensor, n : int, path : str):\n    \"\"\"\n    Saves the input data and labels as a PyTorch tensor file.\n\n    Args:\n        X (tensor): The input data to be saved (e.g., video frames, features).\n        y (tensor): The labels corresponding to the input data.\n        n (int): A numerical identifier that will be used to name the saved file.\n        path (str): The directory path where the tensor file will be saved.\n\n    Saves:\n        A tensor file named 'tensor_{n}.pt' containing the input data (X) and labels (y) \n        at the specified path. The directory is created if it doesn't exist.\n\n    Example:\n        pickle_data(X_data, y_labels, 1, '/path/to/save')\n    \"\"\"\n    tensor_data = {\n        'video': X,\n        'label': y\n    }\n    os.makedirs(path, exist_ok=True)\n    torch.save(tensor_data, os.path.join(path, f'tensor_{n}.pt'))\n</code></pre>"},{"location":"utils/pickle/pickle_data_path_lists.html","title":"<code>pickle_data_path_lists</code>","text":"<p>Save data from a dataloader to disk as separate tensor files.</p>"},{"location":"utils/pickle/pickle_data_path_lists.html#description","title":"Description","text":"<p>The function iterates over a dataloader that yields tuples of video tensors and their corresponding label tensors. Each tuple is saved to disk as a <code>.pt</code> file in the specified directory. The directory will be created if it does not already exist.</p>"},{"location":"utils/pickle/pickle_data_path_lists.html#function-definition","title":"Function Definition","text":"<pre><code>utils.pickle.pickle_data_path_lists(path, dataloader)\n</code></pre>"},{"location":"utils/pickle/pickle_data_path_lists.html#arguments","title":"Arguments","text":"<ul> <li> <p><code>path</code> (<code>str</code>):   The directory where the tensor files will be saved. If the directory does not exist, it will be created automatically.</p> </li> <li> <p><code>dataloader</code> (<code>iterable</code>):   An iterable (e.g., a PyTorch DataLoader) that yields tuples <code>(X, y)</code>.  </p> </li> <li><code>X</code>: The video tensor.  </li> <li><code>y</code>: The corresponding label tensor.</li> </ul>"},{"location":"utils/pickle/pickle_data_path_lists.html#behavior","title":"Behavior","text":"<ul> <li>Each item in the dataloader is saved as a separate <code>.pt</code> file in the specified directory.</li> <li>Filenames are formatted as <code>tensor_&lt;index&gt;.pt</code>, where <code>&lt;index&gt;</code> corresponds to the index of the item in the dataloader.</li> </ul>"},{"location":"utils/pickle/pickle_data_path_lists.html#example-usage","title":"Example Usage","text":"<pre><code>from torch.utils.data import DataLoader\nimport torch\nimport os\n\n# Example dataloader\ndata = [ \n    (torch.randn(3, 224, 224), torch.tensor(0)), \n    (torch.randn(3, 224, 224), torch.tensor(1))\n]\ndataloader = DataLoader(data, batch_size=1)\n\n# Save tensors to disk\npickle_data_path_lists(\"./output_tensors\", dataloader)\n</code></pre>"},{"location":"utils/pickle/pickle_data_path_lists.html#saved-file-structure","title":"Saved File Structure","text":"<p>For a <code>path</code> of <code>./output_tensors</code> and a dataloader containing 3 samples: <pre><code>./output_tensors/\n    tensor_0.pt\n    tensor_1.pt\n    tensor_2.pt\n</code></pre></p> <p>Each <code>.pt</code> file contains a dictionary with two keys: - <code>video</code>: The video tensor. - <code>label</code>: The label tensor.</p>"},{"location":"utils/pickle/pickle_data_path_lists.html#notes","title":"Notes","text":"<ul> <li>The <code>os.makedirs</code> function ensures the directory exists or is created automatically.</li> <li>Files are saved using <code>torch.save</code>, which serializes the tensors to the <code>.pt</code> format.</li> </ul>"},{"location":"utils/pickle/pickle_data_path_lists.html#implementation","title":"Implementation","text":"<pre><code>def pickle_data_path_lists(path : str, dataloader):\n    \"\"\"\n    Save data from a dataloader to disk as separate tensor files.\n\n    Args:\n        path (str): Directory where the tensor files will be saved.\n        dataloader (iterable): An iterable that yields tuples (X, y), where\n            X is the video tensor and y is the corresponding label tensor.\n\n    Each item from the dataloader will be saved as a .pt file in the specified\n    directory, with filenames formatted as 'tensor_&lt;index&gt;.pt'.\n    The directory will be created if it does not already exist.\n    \"\"\"\n    for n, (X, y) in enumerate(dataloader):\n        tensor_data = {\n            'video': X,\n            'label': y\n        }\n        os.makedirs(path, exist_ok=True)\n        torch.save(tensor_data, os.path.join(path, f'tensor_{n}.pt'))\n</code></pre>"},{"location":"utils/pickle/preload_tensors.html","title":"<code>preload_tensors</code>","text":"<p>Loads tensors from a list of file paths and transfers them to the specified device.</p>"},{"location":"utils/pickle/preload_tensors.html#description","title":"Description","text":"<p>This function loads tensor data from a list of file paths and transfers both the input tensors (e.g., video data) and label tensors to the specified device (e.g., CPU or GPU). It is useful for preparing data for machine learning models. (Only use when dataset smaller than Device RAM)!</p>"},{"location":"utils/pickle/preload_tensors.html#function-definition","title":"Function Definition","text":"<pre><code>utils.pickle.preload_tensors(path_list, device)\n</code></pre>"},{"location":"utils/pickle/preload_tensors.html#arguments","title":"Arguments","text":"<ul> <li><code>path_list</code> (<code>list</code>):   A list of file paths to the tensor files to be loaded.</li> <li><code>device</code> (<code>torch.device</code>):   The device (CPU or CUDA) to which the tensors will be transferred.</li> </ul>"},{"location":"utils/pickle/preload_tensors.html#behavior","title":"Behavior","text":"<p>The function iterates over the list of file paths, loading tensor dictionaries from each file. The dictionaries are expected to contain two keys:</p> <ul> <li><code>'video'</code>: The input tensor (e.g., video frames or features).</li> <li><code>'label'</code>: The label tensor corresponding to the input.</li> </ul> <p>Each tensor is transferred to the specified device and stored as a tuple of <code>(video, label)</code> in a list.</p>"},{"location":"utils/pickle/preload_tensors.html#detailed-steps","title":"Detailed Steps","text":"<ol> <li>Initialize an empty list <code>data</code>.</li> <li>For each file path in <code>path_list</code>, load the tensor dictionary using <code>read_pickle_dict</code>.</li> <li>Extract the <code>'video'</code> and <code>'label'</code> tensors and transfer them to the specified device using <code>.to(device)</code>.</li> <li>Append the tuple <code>(video, label)</code> to <code>data</code>.</li> <li>Return the <code>data</code> list.</li> </ol>"},{"location":"utils/pickle/preload_tensors.html#example-usage","title":"Example Usage","text":"<pre><code>import torch\n\ndef preload_tensors(path_list, device):\n    \"\"\"\n    Loads tensors from a list of file paths and transfers them to the specified device.\n\n    Args:\n        path_list (list): A list of file paths to the tensor files to be loaded.\n        device (torch.device): The device (CPU or CUDA) to which the tensors will be transferred.\n\n    Returns:\n        list: A list of tuples, where each tuple contains:\n            - video (torch.tensor): The input data (e.g., video frames, features) loaded and transferred to the specified device.\n            - label (torch.tensor): The corresponding labels, loaded and transferred to the specified device.\n    \"\"\"\n    data = []\n    for path in path_list:\n        tensor = read_pickle_dict(path)  # Load tensor dict from disk\n        video, label = tensor['video'].to(device), tensor['label'].to(device) # Move the tensors to device\n        data.append((video, label))\n    return data\n\n# Example usage\ndata = preload_tensors(['/path/to/file1.pt', '/path/to/file2.pt'], torch.device('cuda'))\nprint(data)\n</code></pre>"},{"location":"utils/pickle/preload_tensors.html#notes","title":"Notes","text":"<ul> <li>Ensure the tensor files are saved in a format compatible with <code>read_pickle_dict</code>.</li> <li>The tensors in the dictionary must have keys <code>'video'</code> and <code>'label'</code> for this function to work correctly.</li> <li>The function assumes that the tensor files can be loaded into memory; for very large datasets, consider alternative approaches such as lazy loading or batching.</li> </ul>"},{"location":"utils/pickle/preload_tensors.html#implementation","title":"Implementation","text":"<pre><code>import torch\n\ndef preload_tensors(path_list, device):\n    \"\"\"\n    Loads tensors from a list of file paths and transfers them to the specified device.\n\n    Args:\n        path_list (list): A list of file paths to the tensor files to be loaded.\n        device (torch.device): The device (CPU or CUDA) to which the tensors will be transferred.\n\n    Returns:\n        list: A list of tuples, where each tuple contains:\n            - video (torch.tensor): The input data (e.g., video frames, features) loaded and transferred to the specified device.\n            - label (torch.tensor): The corresponding labels, loaded and transferred to the specified device.\n    \"\"\"\n    data = []\n    for path in path_list:\n        tensor = read_pickle_dict(path)  # Load tensor dict from disk\n        video, label = tensor['video'].to(device), tensor['label'].to(device) # Move the tensors to device\n        data.append((video, label))\n    return data\n</code></pre>"},{"location":"utils/preload/preload_data.html","title":"<code>preload_data</code>","text":"<p>Transfers batches of data from a DataLoader to the specified device, to avoid repeated loading of data between epochs.</p>"},{"location":"utils/preload/preload_data.html#description","title":"Description","text":"<p>This function takes a PyTorch DataLoader and transfers its batches of data to a specified device (CPU or CUDA). It is useful for preparing data for training or inference on a specific hardware accelerator. (Only use when dataset smaller than Device RAM)!</p>"},{"location":"utils/preload/preload_data.html#function-definition","title":"Function Definition","text":"<pre><code>utils.preload.preload_data(dataloader, device) -&gt; list\n</code></pre>"},{"location":"utils/preload/preload_data.html#arguments","title":"Arguments","text":"<ul> <li> <p><code>dataloader</code> (<code>torch.utils.data.DataLoader</code>):   The DataLoader that provides batches of data.</p> </li> <li> <p><code>device</code> (<code>torch.device</code>):   The device (CPU or CUDA) to which the data batches will be transferred.</p> </li> </ul>"},{"location":"utils/preload/preload_data.html#behavior","title":"Behavior","text":"<p>This function processes each batch from the DataLoader by transferring the input data (<code>X</code>) and the corresponding labels (<code>y</code>) to the specified device. These processed batches are collected into a list of tuples for further use in training or inference workflows.</p>"},{"location":"utils/preload/preload_data.html#example-usage","title":"Example Usage","text":"<pre><code>import torch\nfrom torch.utils.data import DataLoader\n\n# Assuming `train_dataloader` is a predefined DataLoader\ndata = preload_data(train_dataloader, torch.device('cuda'))\nfor X, y in data:\n    print(X.shape, y.shape)\n</code></pre>"},{"location":"utils/preload/preload_data.html#notes","title":"Notes","text":"<ul> <li>Ensure the DataLoader is properly defined and yields batches as tuples of <code>(X, y)</code>.</li> <li>The <code>device</code> must be compatible with the tensors (e.g., ensure CUDA is available for GPU usage).</li> </ul>"},{"location":"utils/preload/preload_data.html#implementation","title":"Implementation","text":"<pre><code>def preload_data(dataloader, device) -&gt; list:\n    \"\"\"\n    Transfers batches of data from a DataLoader to the specified device.\n\n    Args:\n        dataloader (torch.utils.data.DataLoader): The DataLoader that provides batches of data.\n        device (torch.device): The device (CPU or CUDA) to which the data batches will be transferred.\n\n    Returns:\n        list: A list of tuples, where each tuple contains:\n            - X (torch.tensor): The input data (e.g., video frames, features) from the batch, transferred to the specified device.\n            - y (torch.tensor): The corresponding labels from the batch, transferred to the specified device.\n\n    Example:\n        data = preload_data(train_dataloader, torch.device('cuda'))\n    \"\"\"\n    data_tensor_list = []\n    for X, y in dataloader:\n        data_tensor_list.append((X.to(device), y.to(device)))\n    return data_tensor_list\n</code></pre>"},{"location":"utils/save_load/load_checkpoint.html","title":"<code>load_checkpoint</code>","text":"<p>Load the model and optimizer state from a checkpoint file.</p>"},{"location":"utils/save_load/load_checkpoint.html#description","title":"Description","text":"<p>The function loads the state of the model, optimizer, and any other relevant training information from a checkpoint file. This is useful for resuming training or evaluating a previously saved model.</p>"},{"location":"utils/save_load/load_checkpoint.html#function-definition","title":"Function Definition","text":"<pre><code>utils.save_load.load_checkpoint(filename=\"checkpoint.pth\") -&gt; dict\n</code></pre>"},{"location":"utils/save_load/load_checkpoint.html#arguments","title":"Arguments","text":"<ul> <li><code>filename</code> (<code>str</code>, optional):   The name of the file from which the checkpoint will be loaded. Default is <code>'checkpoint.pth'</code>.</li> </ul>"},{"location":"utils/save_load/load_checkpoint.html#returns","title":"Returns","text":"<ul> <li><code>dict</code>:   A dictionary containing:</li> <li><code>model_state_dict</code>: The state dictionary of the model.</li> <li><code>optimizer_state_dict</code>: The state dictionary of the optimizer.</li> <li>Other relevant training information (e.g., epoch number).</li> </ul>"},{"location":"utils/save_load/load_checkpoint.html#example-usage","title":"Example Usage","text":"<pre><code>import torch\n\n# Example model and optimizer\nmodel = torch.nn.Linear(10, 1)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Load the checkpoint\ncheckpoint = load_checkpoint('model_checkpoint.pth')\n\n# Restore model and optimizer state\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n</code></pre>"},{"location":"utils/save_load/load_checkpoint.html#notes","title":"Notes","text":"<ul> <li>The function uses <code>torch.load</code> to deserialize the checkpoint file.</li> <li>Ensure that the checkpoint file exists and matches the expected format before calling this function.</li> <li>Useful for both resuming interrupted training sessions and evaluating saved models.</li> </ul>"},{"location":"utils/save_load/load_checkpoint.html#implementation","title":"Implementation","text":"<pre><code>import torch\n\ndef load_checkpoint(filename=\"checkpoint.pth\") -&gt; dict:\n    \"\"\"\n    Loads the model and optimizer state from a checkpoint file.\n\n    Args:\n        filename (str, optional): The name of the file from which the checkpoint will be loaded. Default is 'checkpoint.pth'.\n\n    Returns:\n        dict: A dictionary containing the model state, optimizer state, and any other relevant training information.\n\n    Example:\n        checkpoint = load_checkpoint('model_checkpoint.pth')\\\\\n        model.load_state_dict(checkpoint['model_state_dict'])\\\\\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \"\"\"\n    checkpoint = torch.load(filename)\n    return checkpoint\n</code></pre>"},{"location":"utils/save_load/load_model.html","title":"<code>load_model</code>","text":"<p>Load the model state from a checkpoint and update the model.</p>"},{"location":"utils/save_load/load_model.html#description","title":"Description","text":"<p>The function updates the provided model with the state stored in a checkpoint file. This is useful for restoring a model's parameters for further training or evaluation.</p>"},{"location":"utils/save_load/load_model.html#function-definition","title":"Function Definition","text":"<pre><code>utils.save_load.load_model(model, checkpoint)\n</code></pre>"},{"location":"utils/save_load/load_model.html#arguments","title":"Arguments","text":"<ul> <li> <p><code>model</code> (<code>torch.nn.Module</code>):   The PyTorch model instance to load the state into.</p> </li> <li> <p><code>checkpoint</code> (<code>dict</code>):   A dictionary containing the model state, typically loaded using a function like <code>load_checkpoint</code>. It should have the key <code>model_state_dict</code> which stores the model's parameters.</p> </li> </ul>"},{"location":"utils/save_load/load_model.html#returns","title":"Returns","text":"<ul> <li><code>torch.nn.Module</code>:   The model with the loaded state.</li> </ul>"},{"location":"utils/save_load/load_model.html#example-usage","title":"Example Usage","text":"<pre><code>import torch\n\n# Example model\nmodel = torch.nn.Linear(10, 1)\n\n# Example checkpoint loaded from a file\ncheckpoint = load_checkpoint('model_checkpoint.pth')\n\n# Load the model state\nmodel = load_model(model, checkpoint)\n</code></pre>"},{"location":"utils/save_load/load_model.html#notes","title":"Notes","text":"<ul> <li>Ensure that the checkpoint dictionary contains a <code>model_state_dict</code> key corresponding to the model's state.</li> <li>The function uses <code>model.load_state_dict</code> to update the model's parameters.</li> </ul>"},{"location":"utils/save_load/load_model.html#implementation","title":"Implementation","text":"<pre><code>import torch\n\ndef load_model(model, checkpoint):\n    \"\"\"\n    Loads the model state from a checkpoint and updates the model.\n\n    Args:\n        model (torch.nn.Module): The model to load the state into.\n        checkpoint (dict): A dictionary containing the model state, typically loaded from a checkpoint file.\n\n    Returns:\n        torch.nn.Module: The model with the loaded state.\n\n    Example:\n        checkpoint = load_checkpoint('model_checkpoint.pth')\n        model = load_model(model, checkpoint)\n    \"\"\"\n    model.load_state_dict(checkpoint['model_state_dict'])\n    return model\n</code></pre>"},{"location":"utils/save_load/save_checkpoint.html","title":"<code>save_checkpoint</code>","text":"<p>Save the model and optimizer state to a specified file.</p>"},{"location":"utils/save_load/save_checkpoint.html#description","title":"Description","text":"<p>The function saves the state of the model, optimizer, and any other relevant training information to a file. This is useful for checkpointing during training, allowing you to resume training or evaluate the model later.</p>"},{"location":"utils/save_load/save_checkpoint.html#function-definition","title":"Function Definition","text":"<pre><code>utils.save_load.save_checkpoint(state, filename=\"checkpoint.pth\")\n</code></pre>"},{"location":"utils/save_load/save_checkpoint.html#arguments","title":"Arguments","text":"<ul> <li><code>state</code> (<code>dict</code>):   A dictionary containing:</li> <li><code>model_state_dict</code>: The state dictionary of the model.</li> <li><code>optimizer_state_dict</code>: The state dictionary of the optimizer.</li> <li> <p>Other relevant training information (e.g., epoch number).</p> </li> <li> <p><code>filename</code> (<code>str</code>, optional):   The name of the file where the checkpoint will be saved. Default is <code>'checkpoint.pth'</code>.</p> </li> </ul>"},{"location":"utils/save_load/save_checkpoint.html#example-usage","title":"Example Usage","text":"<pre><code>import torch\n\n# Example model and optimizer\nmodel = torch.nn.Linear(10, 1)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Create a checkpoint dictionary\nstate = {\n    'epoch': 10,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict()\n}\n\n# Save the checkpoint\nsave_checkpoint(state, 'model_checkpoint.pth')\n</code></pre>"},{"location":"utils/save_load/save_checkpoint.html#saved-file-structure","title":"Saved File Structure","text":"<p>The saved file (e.g., <code>model_checkpoint.pth</code>) contains a dictionary with keys like: - <code>epoch</code>: The epoch number at which the checkpoint was created. - <code>model_state_dict</code>: The serialized model parameters. - <code>optimizer_state_dict</code>: The serialized optimizer parameters.</p>"},{"location":"utils/save_load/save_checkpoint.html#notes","title":"Notes","text":"<ul> <li>Use <code>torch.load</code> to load the saved checkpoint when resuming training or for evaluation.</li> <li>The function ensures that training progress is not lost in case of interruptions.</li> </ul>"},{"location":"utils/save_load/save_checkpoint.html#implementation","title":"Implementation","text":"<pre><code>import torch\n\ndef save_checkpoint(state, filename=\"checkpoint.pth\"):\n    \"\"\"\n    Saves the model and optimizer state to a specified file.\n\n    Args:\n        state (dict): A dictionary containing the model state, optimizer state, and any other relevant training information.\n        filename (str, optional): The name of the file where the checkpoint will be saved. Default is 'checkpoint.pth'.\n\n    Example:\n        save_checkpoint({'epoch': 10, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, 'model_checkpoint.pth')\n    \"\"\"\n    torch.save(state, filename)\n</code></pre>"},{"location":"utils/show_sequence/show_sequence.html","title":"<code>show_sequence</code>","text":"<p>Display a sequence of frames in a grid format.</p>"},{"location":"utils/show_sequence/show_sequence.html#description","title":"Description","text":"<p>The function visualizes a sequence of frames as a grid using <code>matplotlib</code>. Each frame is displayed as an image, with the sequence organized into rows and columns for easy visualization. The frames are normalized for display.</p>"},{"location":"utils/show_sequence/show_sequence.html#function-definition","title":"Function Definition","text":"<pre><code>utils.show_sequence.show_sequence(sequence, NUM_FRAMES)\n</code></pre>"},{"location":"utils/show_sequence/show_sequence.html#arguments","title":"Arguments","text":"<ul> <li> <p><code>sequence</code> (<code>torch.tensor</code>):   A tensor containing frames to be displayed. Each frame should be a 3D tensor of shape <code>(C, H, W)</code> representing a color image.</p> </li> <li> <p><code>NUM_FRAMES</code> (<code>int</code>):   The total number of frames in the sequence to be displayed.</p> </li> </ul>"},{"location":"utils/show_sequence/show_sequence.html#returns","title":"Returns","text":"<ul> <li><code>None</code>:   The function directly displays the frames using <code>matplotlib</code>.</li> </ul>"},{"location":"utils/show_sequence/show_sequence.html#example-usage","title":"Example Usage","text":"<pre><code>import torch\n\n# Example sequence: 16 frames of size (3, 64, 64)\nsequence = torch.randn(16, 3, 64, 64)\nNUM_FRAMES = 16\n\n# Display the sequence\nshow_sequence(sequence, NUM_FRAMES)\n</code></pre>"},{"location":"utils/show_sequence/show_sequence.html#notes","title":"Notes","text":"<ul> <li>The grid layout defaults to 4 columns. The number of rows is calculated dynamically based on the number of frames.</li> <li>The function normalizes each frame by dividing by its maximum value to ensure proper visualization.</li> <li>Ensure that <code>sequence</code> contains at least <code>NUM_FRAMES</code> frames to avoid indexing errors.</li> <li><code>matplotlib</code>'s <code>GridSpec</code> is used for flexible grid layouts.</li> </ul>"},{"location":"utils/show_sequence/show_sequence.html#visualization-example","title":"Visualization Example","text":"<p>For a sequence of 16 frames: - Grid Layout: 4 columns x 4 rows - Each frame is displayed as an image with no axis markers.</p> <p></p>"},{"location":"utils/show_sequence/show_sequence.html#implementation","title":"Implementation","text":"<pre><code>import os\nfrom matplotlib import pyplot as plt\nimport matplotlib.gridspec as gridspec\n\ndef show_sequence(sequence, NUM_FRAMES):\n    \"\"\"\n    Displays a sequence of frames in a grid format.\n\n    Args:\n        sequence (torch.tensor): A tensor containing frames to be displayed. \n                                        Each frame should be a 3D tensor (C, H, W) representing a color image.\n        NUM_FRAMES (int): The total number of frames the tensor contains.\n\n    Returns:\n        None: The function directly displays the frames using `matplotlib`.\n\n    Example:\n        show_sequence(frame_sequence, 16)\n    \"\"\"\n    columns = 4\n    rows = (NUM_FRAMES + 1) // (columns)\n    fig = plt.figure(figsize=(32, (16 // columns) * rows))\n    gs = gridspec.GridSpec(rows, columns)\n    for j in range(rows * columns):\n        plt.subplot(gs[j])\n        plt.axis(\"off\")\n        frames = sequence[j].permute(1,2,0).numpy()\n        frames = frames/ frames.max()\n        plt.imshow(frames)\n    plt.show()\n</code></pre>"}]}